{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7025b978",
   "metadata": {},
   "source": [
    "# NLTK - Corpora\n",
    "\n",
    "A **text corpus** is a dataset containing **natural language** text from common sources, compiled to represent a specific domain or aspect of language. These often include books, transcripts, websites, correspondence, etc. They can also be annotated with linguistic features, and contain text in many languages.\n",
    "\n",
    "A corpus should be large, principled, and authentic.\n",
    "\n",
    "Unlike a simple database dump of text data (such as those offered by Wikipedia), corpora are curated to be useful in linguistic analysis, often offering balanced representation of different genres or languages, and human annotation.\n",
    "\n",
    "- How can they be useful? What kind of projects would you use a corpus for?\n",
    "- Types of corpora? diachronic/synchronic/monitor, monolingual/multilingual/parallel, raw/tagged, learner corpus, error-tagged corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531bafa",
   "metadata": {},
   "source": [
    "## 1. Exploring Corpora\n",
    "\n",
    "NLTK enables us to access pre-compiled corpora. We will open and explore four of them today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa30ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg, inaugural, brown, dependency_treebank\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33be1c",
   "metadata": {},
   "source": [
    "We download the four corpora we will study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Gutenberg Corpus contains several literary texts from Project Gutenberg.\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# The Inaugural Address Corpus contains the texts of the inaugural addresses of U.S. presidents.\n",
    "nltk.download('inaugural')\n",
    "\n",
    "# The Brown Corpus is a standard corpus of American English texts.\n",
    "nltk.download('brown')\n",
    "\n",
    "# The Dependency Treebank Corpus contains sentences annotated with their syntactic structure.\n",
    "nltk.download('dependency_treebank')\n",
    "\n",
    "# The Punkt tokenizer is used for sentence splitting.\n",
    "nltk.download('punkt')  # For tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28986a2",
   "metadata": {},
   "source": [
    "And print the files that make them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print files in each corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd50e6",
   "metadata": {},
   "source": [
    "### 1.1. Analysing the Gutenberg Corpus\n",
    "\n",
    "Let's dive into one of the texts and perform some simple explorative analysis.\n",
    "\n",
    "We can load the dataset in several forms, including the raw characters, list of words, or list of sentences, like so:\n",
    "\n",
    "```\n",
    "raw_text = gutenberg.raw()\n",
    "words = gutenberg.words()\n",
    "sentences = gutenberg.sents()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw text of Alice in Wonderland using .raw()\n",
    "\n",
    "# print first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .words() method to access the text as a list of words\n",
    "\n",
    "# print first 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .sents() method to access the sentences\n",
    "\n",
    "# print two sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31500b86",
   "metadata": {},
   "source": [
    "Let's print some statistics about the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out number of unique words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17db962",
   "metadata": {},
   "source": [
    "Let's perform some frequency analysis on Alice in Wonderland. You will use `FreqDist` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print frequency distribution words in Alice in Wonderland\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 30 most common words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ee402",
   "metadata": {},
   "source": [
    "Most of these aren't very insightful. Punctuation and function words are very common in all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513afe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter punctuation and stopwords for better visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881807a",
   "metadata": {},
   "source": [
    "Why and when to filter stopwords and punctuation? what about capitalisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e28347",
   "metadata": {},
   "source": [
    "### 1.2. ✏️ Your turn!\n",
    "\n",
    "Try performing a similar exploration of the `inaugural` or `brown` corpus. Any surprises? What about if you restrict it to one of their files or categories?\n",
    "\n",
    "For either the entire corpus or one of the files/categories, print:\n",
    "\n",
    "- The number of words\n",
    "- The 30 most common words\n",
    "- The 30 most common words, filtering out stopwords and punctuation\n",
    "\n",
    "Extra:\n",
    "- Find \"Hapax legomenons\": words that occur only once in a corpus. Hint: `fdist['word']` returns the count for that word. So you can do `for word in fdist.keys():`...\n",
    "- Normalise capitalisation, so that \"The\" and \"the\" are counted as the same word. Hint: in Python, you can do `word.lower()` to get the lowercase version of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✏️ STUDENTS: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303db614",
   "metadata": {},
   "source": [
    "What can we learn by exploring a corpus in this way? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0d439",
   "metadata": {},
   "source": [
    "### 1.3. Comparative Analysis\n",
    "\n",
    "What can comparing different corpora or categories within a corpus tell us?\n",
    "\n",
    "Finding differences between texts of different genres, authors, or periods is called **stylistics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use FreqDist to compute proportions of \"said\" per Gutenberg file\n",
    "# Fill code here\n",
    "\n",
    "# plot the proportion of \"said\" in each Gutenberg file as a barplot\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.bar(gutenberg.fileids(), fd_gutenberg)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use FreqDist to compute counts of \"states\" and \"america\" per inaugural address\n",
    "\n",
    "# IDs look like '1789-Washington.txt', so we extract the year part\n",
    "address_ids = inaugural.fileids()\n",
    "years = [fileid[:4] for fileid in address_ids]\n",
    "\n",
    "# Fill code here\n",
    "\n",
    "# plot the occurrence of \"states\" and \"america\" in each inaugural address as a line plot\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(years, fd_inaugural_states, label='united')\n",
    "plt.plot(years, fd_inaugural_america, label='america')\n",
    "plt.xticks(rotation=60)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa73976",
   "metadata": {},
   "source": [
    "Noticed how we used proportions with the Gutenberg corpus, but total counts with the inaugural corpus? What are the pros and cons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use FreqDist to compute proportions of \"will\" per Brown file\n",
    "# Fill code here\n",
    "\n",
    "# plot the proportion of \"will\" in each Brown file as a barplot\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.bar(brown.categories(), fd_brown)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c331bbd",
   "metadata": {},
   "source": [
    "What happens if we switch to \"could\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee331572",
   "metadata": {},
   "source": [
    "### 1.4. ✏️ Test your own hypothesis\n",
    "\n",
    "Use the examples above to come up with your own comparative hypothesis and test it. Some examples:\n",
    "- Are words in a certain category of Brown longer on average than in another one?\n",
    "- Does an antiquated term fade out of use in Inauguration as the years go by?\n",
    "- Do poems use a certain word more frequently than novels? what about plays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad000e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✏️ STUDENTS: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94414903",
   "metadata": {},
   "source": [
    "## 2. Annotated Corpora\n",
    "\n",
    "Corpora are often annotated with linguistic features. Which ones do you think would be useful, and for what tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85548588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Brown corpus annotations: print tagged words in the \"news category\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1985a",
   "metadata": {},
   "source": [
    "Do you recognise these tags? https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927d971",
   "metadata": {},
   "source": [
    "Let's look at another corpus, this time it's tagged with a _Dependency Parse_. Such corpora are called \"treebanks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff93b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependency Treebank corpus, format as conll, and print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84449aba",
   "metadata": {},
   "source": [
    "## 3. ✏️ Creating your own Corpus\n",
    "\n",
    "NLTK allows us to build our own corpus from text files. Find a book you like at https://www.gutenberg.org/ebooks/categories, and open it as \"Plain Text UTF-8\". Hit `Ctrl+S` or `Cmd+S` to save it in the same directory as this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a txt file using PlaintextCorpusReader, and print fileids\n",
    "my_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82df999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print excerpt from my corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef5316",
   "metadata": {},
   "source": [
    "Now, use the code from earlier to print some statistics about your file:\n",
    "- The 30 most common words, filtering out stopwords and punctuation, and ignoring capitalisation\n",
    "- Find the most common word in the Brown corpus that does not appear in your corpus. _Hint_: You can create a `FreqDist` of both corpora, and use `for word, count in fdist.most_common(100)` to find it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✏️ STUDENTS: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc530174",
   "metadata": {},
   "source": [
    "### ✏️ Extra activity: Zipf's Law\n",
    "\n",
    "Verify Zipf's law using different corpora. Zipf's law states that the frequency of the Nth most common word is proportional to 1/N, so the second most common word appears 1/2 as many times as the first, and the third appears 1/3 as many times as the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ad222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FreqDist of the corpus, filtering out stopwords and punctuation, and ignoring capitalisation\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ranks, frequencies, marker='o')\n",
    "plt.plot(ranks, [frequencies[0]/rank for rank in ranks], linestyle='dashed', color='red', marker='D')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "#plt.yscale(\"log\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bf2f7",
   "metadata": {},
   "source": [
    "Now try yours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69883f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✏️ STUDENTS: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3479c2f",
   "metadata": {},
   "source": [
    "## 4. Generating Random Text with Bigrams\n",
    "\n",
    "Bigrams are word pairs that appear consequtively in a text. So the text \"I had some coffee\" has 3 bigrams, (I had), (had some), and (some coffee).\n",
    "\n",
    "- Have you encountered bigrams before? What about N-grams?\n",
    "- How can they be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3120ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that given a word will output a likely next word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function with the Brown corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173be840",
   "metadata": {},
   "source": [
    "Now try it with your own corpus! Anything interesting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54445914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✏️ STUDENTS: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e4847",
   "metadata": {},
   "source": [
    "Some questions:\n",
    "- What are the limitations of bigrams? \n",
    "- How big should N be so that N-grams are useful? \n",
    "- How do modern machine learning models differ from N-grams?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
